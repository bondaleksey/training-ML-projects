{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating through the list of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from parsing_page import parsing_author_page, parsing_article_page\n",
    "import time\n",
    "import requests\n",
    "from os.path import exists\n",
    "from os import makedirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_types import Author, AuthorsDB\n",
    "from data_types import Publication, PublicationsDB\n",
    "from data_types import Abstract, AbstractsDB\n",
    "audb = AuthorsDB()\n",
    "# audb.load()\n",
    "pubdb = PublicationsDB()\n",
    "# pubdb.load()\n",
    "absdb = AbstractsDB()\n",
    "# absdb.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/mathnet_iam_authors_dict.pkl\"\n",
    "with open(filename,'rb') as inp:\n",
    "    authors_dict = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def need_to_scrap(author, authors_db:AuthorsDB):\n",
    "    interesting_list = [\"Бондаренко\", \"Поляков\", \"Якобовский\", \"Четверушкин\"]\n",
    "    # interesting_list = [\"Бондаренко\", \"Якобовский\"]\n",
    "    # interesting_list = [\"Бондаренко\"]\n",
    "    for person in interesting_list:\n",
    "        if person in author['name']:\n",
    "            # we can demand some other conditions\n",
    "            # if not authors_db.check_key(author[\"mn_id\"]):\n",
    "            if authors_db.check_key(author[\"mn_id\"]):\n",
    "                print(f\"We have him {author} in authors_db\")\n",
    "                return True\n",
    "            else:\n",
    "                # print(f\"We have him {author} in authors_db\")\n",
    "                print(f\"We don't have him {author} in authors_db\")\n",
    "                return True\n",
    "    return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(data, mnid, name, status='write'):\n",
    "    newpath = r'../data/'+mnid+'/'\n",
    "    if not exists(newpath):\n",
    "        makedirs(newpath)\n",
    "    filename = \"../data/\"+mnid+\"/\"+name+\".pkl\"\n",
    "    if status == 'write':        \n",
    "        with open(filename,'wb') as outp:\n",
    "            pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_info(mnid, status=\"not_saving\", web='offline'):\n",
    "    print(\"../data/\"+mnid+\"/mn_author_\"+mnid+\"_page.pkl\")\n",
    "    print(exists(\"../data/\"+mnid+\"/mn_author_\"+mnid+\"_page.pkl\"))\n",
    "    if exists(\"../data/\"+mnid+\"/mn_author_\"+mnid+\"_page.pkl\"):\n",
    "        print(\"We read from file\")\n",
    "        filename = \"../data/\"+mnid+\"/mn_author_\"+mnid+\"_page.pkl\"\n",
    "        with open(filename,'rb') as inp:\n",
    "            author_page = pickle.load(inp)\n",
    "    else:\n",
    "        print(f\"we need to request page for author mnid = {mnid}\")\n",
    "        if web=='offline':\n",
    "            return None\n",
    "        page_link = f\"http://www.mathnet.ru/php/person.phtml?option_lang=rus&personid={mnid}\"\n",
    "        \n",
    "        # Chancge 'User-Agent' HEADERRRRR after some time of use !!!!\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        # page_link =\"http://mi.mathnet.ru/+ {}\"\n",
    "        time.sleep(0.7)\n",
    "        response = requests.get(page_link, headers=headers, timeout=None)\n",
    "        if response.status_code == 200:\n",
    "            author_page = response.content\n",
    "        if status==\"saving\":\n",
    "            save_html(author_page, mnid, \"mn_author_\"+mnid+\"_page\",status='write')\n",
    "        author_page\n",
    "                    \n",
    "    soup = BeautifulSoup(author_page, 'html.parser')\n",
    "    # TODO:\n",
    "    # Need to check if this is real page and not for bot response\n",
    "    return parsing_author_page(soup) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pub_info(mn_id, mnlink, status=\"not_saving\",web='offline'):\n",
    "    print(\"../data/\"+mn_id+\"/\"+mnlink+\".pkl\")\n",
    "    print(exists(\"../data/\"+mn_id+\"/\"+mnlink+\".pkl\"))\n",
    "    if exists(\"../data/\"+mn_id+\"/\"+mnlink+\".pkl\"):\n",
    "        print(\"We read from file\")\n",
    "        filename = \"../data/\"+mn_id+\"/\"+mnlink+\".pkl\"\n",
    "        with open(filename,'rb') as inp:\n",
    "            pub_page = pickle.load(inp)\n",
    "    else:\n",
    "        print(\"we don't find that file\")\n",
    "        if web == 'offline':\n",
    "            return None   \n",
    "        print(f\"we need to request page for pub_page mnlink = {mnlink}\")\n",
    "        page_link = \"http://mi.mathnet.ru/\" + mnlink\n",
    "        print(page_link)\n",
    "        # Chancge 'User-Agent' HEADERRRRR after some time of use !!!!\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        # \n",
    "        time.sleep(0.5)\n",
    "        response = requests.get(page_link, headers=headers, timeout=None)        \n",
    "        print(response.status_code)        \n",
    "        if response.status_code == 200:\n",
    "            pub_page = response.content            \n",
    "            print(\"we save file: \" + mnlink)\n",
    "            if status==\"saving\":\n",
    "                save_html(pub_page, mn_id, mnlink,status='write')\n",
    "        else:\n",
    "            print(response.status_code)\n",
    "            print(type(response.status_code))\n",
    "            print(\"response.status_code != 200:\")\n",
    "            return \"Stop\"\n",
    "        # pub_page        \n",
    "    soup = BeautifulSoup(pub_page, 'html.parser')\n",
    "    # if status != \"working\":\n",
    "    #     return parsing_article_page(soup)\n",
    "    \n",
    "    return parsing_article_page(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_html_status = \"saving\"\n",
    "status = \"testing\"\n",
    "# safe\n",
    "cur_web = 'offline'\n",
    "#saveingDB = 'easy'\n",
    "# -------------\n",
    "# working\n",
    "cur_web = 'online'\n",
    "saveingDB = 'write'\n",
    "for indx, item in authors_dict.items(): \n",
    "    request_problems = False\n",
    "    if need_to_scrap(item, audb):                \n",
    "        new_author = None\n",
    "        new_author = Author(mn_id=item[\"mn_id\"])\n",
    "        print(item)\n",
    "        # get through pubs list on author page\n",
    "        author_pubs = None\n",
    "        author_pubs = get_author_info(item[\"mn_id\"], status=save_html_status,web=cur_web)\n",
    "        if author_pubs is None:\n",
    "            continue\n",
    "        for pubs_indx, pubs_item in author_pubs.items():\n",
    "            if 'mn_link' not in pubs_item:\n",
    "                continue                \n",
    "            print(pubs_indx, pubs_item)            \n",
    "            if pubs_item[\"mn_link\"] in pubdb.db.keys():\n",
    "                # статья уже есть надо просто добавить к автору\n",
    "                new_author.read_from_pubdb(pubs_item[\"mn_link\"], audb)                \n",
    "                continue\n",
    "            \n",
    "            if pubs_item[\"mn_link\"] is not None:\n",
    "                pub_page_info = None                                \n",
    "                pub_page_info = get_pub_info(item[\"mn_id\"],pubs_item[\"mn_link\"],status=save_html_status,web=cur_web)\n",
    "                # if len(pub_page_info)==1:\n",
    "                # # if pub_page_info==\"Stop\":\n",
    "                #     audb.save(f\"we have len(pub_page_info)==1 \\nsaving after author={item['mn_id']}, link={pubs_item['mn_link']}\")\n",
    "                #     pubdb.save()\n",
    "                #     absdb.save()\n",
    "                #     # request_problems = True\n",
    "                #     continue\n",
    "                \n",
    "                    \n",
    "                new_pub = Publication(pubs_item[\"mn_link\"])\n",
    "                new_abs = Abstract(pubs_item[\"mn_link\"])\n",
    "                \n",
    "                if pub_page_info is not None:                    \n",
    "                    new_author.update_author_info(pubs_item, pub_page_info)\n",
    "                    new_pub.update_publication_info(pubs_item, pub_page_info)\n",
    "                    new_abs.update_abstract_info(pubs_item, pub_page_info)                    \n",
    "                    pubdb.update_data(new_pub.convert2dict())                            \n",
    "                    absdb.update_data(new_abs.convert2dict())\n",
    "                    for page_indx, page_item  in pub_page_info.items():\n",
    "                        print(page_indx)\n",
    "                        print(page_item)                              \n",
    "        if request_problems == True:\n",
    "            break\n",
    "        print(new_author.convert2dict())\n",
    "        audb.update_data(new_author.convert2dict())\n",
    "        if status  == \"testing\":\n",
    "            audb.show()\n",
    "            pubdb.show()\n",
    "            absdb.show()\n",
    "        audb.save(status = saveingDB,text = f\"we saving after {item['mn_id']} author\")\n",
    "        pubdb.save(status = saveingDB)\n",
    "        absdb.save(status = saveingDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubdb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absdb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58602d8dccd692fa32abd17425fd5d45fcdd50ad23d8d4cc69f0d11e0a82e605"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mynetscrap': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
